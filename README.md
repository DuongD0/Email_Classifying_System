# Email Classification

## Overview
This project classifies emails into `NORMAL`, `SPAM`, or `FRAUD` using a classical machine‑learning pipeline built on top of scikit-learn. The workflow covers text cleaning, extensive model benchmarking, evaluation, and artifact persistence so the best performer can be reused for real-time detection.

## Technology Stack
- Python 3.9+
- scikit-learn (model zoo, metrics, vectorizers)
- pandas & numpy (data manipulation)
- nltk (English stopwords)
- matplotlib & seaborn (visualization)
- joblib (model persistence)
- argparse (CLI controls)

## Dataset & Experiment Setup
- Data source: balanced corpus of 3,000 emails (1,000 per class) generated by `Extract_email.py`.
- Train/test split: 70/30 using `train_test_split` with `random_state=111`.
- Feature suites explored:
  - TF-IDF (baseline, no stemming)
  - TF-IDF with stemming
  - TF-IDF + email length feature
  - Count Vectorizer (with and without stemming)
- Model families: SVC, KNN, MultinomialNB, Decision Tree, Logistic Regression, Random Forest, AdaBoost, Bagging, ExtraTrees, SGD (hinge/logistic), and multiple voting ensembles.

## Code Brief
 - This file involves the process of Data Extraction. In this, 1000 fraud emails from the ‘fradulent_emails.txt’ file containing 4075 emails are extracted and, 1000 emails for each Spam and Normal category are extracted from ‘emails.csv’ file that contains 5730 emails which is a combination of both Spam and Normal emails. Finally, all the extracted emails are concatenated into one csv file. This csv file contains the final dataset that contains 3000 emails with 1000 emails for each category.
 - Email_Classification.py:
   This file involves the complete process of email processing and classification:

1. Data Preprocessing:
Functions are created for the removal of punctuation and stopwords. Another function is created for stemming of the content.
In order to extract the relevant features 2 vectors were used: TF-IDF vectors and Count Vectors. First, the entire process of classification is performed by the features created using TF-IDF and then the features created by Count- Vectorizer are processed and observed. Then the features are split into train and test set in the ratio 7:3 respectively.

2. Text Classification:
Various classifers are trained on the features extracted above and then, their performance is observed. Before the training of the models, Parameter Tuning is performed to identify the optimum parameters for each classifier.

3. Evaluation Metrics:
The classifiers are evaluated on the basis of:
<ul>
   <li> Accuracy</li>
   <li> Confusion Matrix</li>
   <li> Precision, Recall and F-Score</li>
</ul>


  Accuracy
  Confusion Matrix
  Precision, Recall and F-Score

## Latest Training Snapshot
_Output from `python Email_Classification.py` on the bundled dataset_

| Feature Block | Best Model | Accuracy | Macro Precision | Macro Recall | Notes |
| --- | --- | --- | --- | --- | --- |
| TF-IDF (no stem) | SVC (sigmoid, γ≈0.95) | **0.9733** | 0.977 | 0.976 | Persisted to `models/svc_classifier.joblib` |
| TF-IDF (no stem) | SGD Logistic | 0.9756 | 0.978 | 0.978 | Close runner-up |
| TF-IDF (stemmed) | MultinomialNB | 0.9733 | 0.973 | 0.973 | |
| TF-IDF (stemmed) | Vote(SVC, BgC, RF) | 0.9744 | 0.975 | 0.974 | |
| Count Vectorizer | MultinomialNB | 0.9733 | 0.973 | 0.973 | |
| Count Vectorizer (stemmed) | MultinomialNB | 0.9756 | 0.976 | 0.976 | |

Confusion-matrix highlights for the persisted SVC (TF-IDF, no stem):
- FRAUD: precision 0.96, recall 0.99
- SPAM: precision 0.97, recall 0.96
- NORMAL: precision 1.00, recall 0.98

The pipeline automatically overwrites `models/tfidf_vectorizer.joblib` and `models/svc_classifier.joblib` with the best TF-IDF model from the baseline block; delete either file to force a full retrain.

## CLI & Workflow
```bash
# End-to-end training (overwrites cached artifacts)
python Email_Classification.py

# Skip training and just load the saved artifacts
python Email_Classification.py --skip-training

# Retrain even if artifacts exist (silences overwrite warning)
python Email_Classification.py --force-retrain
```

### Data Preparation
```bash
python Extract_email.py  # regenerates Datasets/final_dataset.csv
```
Adjust dataset paths inside `Extract_email.py` if your raw corpora live elsewhere.

## Real-Time Detection
```python
from Email_Classification import RealTimeEmailDetector

detector = RealTimeEmailDetector()
label = detector.predict("Congratulations, you've won...")
print(label)  # -> SPAM / FRAUD / NORMAL
```
The helper loads the persisted vectorizer and classifier, applies the same preprocessing (punctuation removal + stopword filtering), and returns a label in real time. Embed this call in a Gmail webhook, IMAP poller, or any service that hands you raw email bodies.

## Troubleshooting & Tips
- **Missing stopwords**: `python -c "import nltk; nltk.download('stopwords')"`
- **Artifacts not updating**: rerun with `--force-retrain` or delete the files in `models/`.
- **Long training runs**: comment out experiment blocks you do not need, or subsample for experimentation.
- **Dataset not found**: ensure `Datasets/final_dataset.csv` exists or update `input_dataset` in `Email_Classification.py`.

For deeper experimentation, tweak hyperparameters inside `Email_Classification.py`, extend preprocessing via `Data_Preprocessing`, or add custom voting configurations. The README will stay current with the latest benchmark results so you can track progress over time.
